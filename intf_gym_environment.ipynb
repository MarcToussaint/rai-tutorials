{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9097e1f3",
   "metadata": {},
   "source": [
    "# Gym Environment Interface: minimal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9101b6db-e9c2-4f29-bc50-de5d3231f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ry version: 0.1.4 compile time: Jan 24 2024 13:12:18\n"
     ]
    }
   ],
   "source": [
    "import robotic as ry\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "print('ry version:', ry.__version__, ry.compiled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f641c445-4901-4d95-9ecd-c931d6b80bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A basic configuration, where the orange box is to be pushed to the target with the stick\n",
    "\n",
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('scenarios/pandaSingle.g'))\n",
    "C.view(False)\n",
    "\n",
    "C.addFrame('box') \\\n",
    "    .setShape(ry.ST.ssBox, size=[.1,.1,.1,.005]) .setColor([1,.5,0]) \\\n",
    "    .setPosition([.1,.35,.9]) \\\n",
    "    .setMass(.1)\n",
    "\n",
    "C.addFrame('stick', 'l_gripper') \\\n",
    "    .setShape(ry.ST.capsule, size=[.3,.02]) .setColor([.5,1,0]) \\\n",
    "    .setRelativePosition([0,0,-.13])\n",
    "\n",
    "C.addFrame('target') \\\n",
    "    .setShape(ry.ST.marker, size=[.1]) .setColor([0,1,0]) \\\n",
    "    .setPosition([.5,.0,.7]) \\\n",
    "\n",
    "C.setJointState([.0], ['l_panda_joint2']) #only cosmetics\n",
    "C.setJointState([.02], ['l_panda_finger_joint1']) #only cosmetics\n",
    "\n",
    "q0 = C.getJointState()\n",
    "X0 = C.getFrameState()\n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e95379-5ff6-4b05-abaf-f356550653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic gym environment, instantiating pyhsx multibody sim, with velocity control\n",
    "# the arguments C, time_limit, and reward_fct define the problem\n",
    "\n",
    "class RaiGym(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    tau = .05\n",
    "    time = 0.\n",
    "\n",
    "    def __init__(self, C, time_limit, reward_fct, render_mode=None):\n",
    "        self.C = C\n",
    "        self.time_limit = time_limit\n",
    "        self.reward_fct = reward_fct\n",
    "        self.render_mode = render_mode\n",
    "        #self.limits = self.C.getJointLimits()\n",
    "        self.limits = [-10., 10.]\n",
    "        self.q0 = self.C.getJointState()\n",
    "        self.X0 = self.C.getFrameState()\n",
    "\n",
    "        self.observation_space = gym.spaces.box.Box(self.limits[0], self.limits[1], shape=(self.q0.size,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.box.Box(low=-1., high=1., shape=(self.q0.size,), dtype=np.float32)\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.sim = ry.Simulation(self.C, ry.SimulationEngine.physx, 0)\n",
    "\n",
    "    def __del__(self):\n",
    "        del self.sim\n",
    "        del self.C\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.sim.step(action, self.tau, ry.ControlMode.velocity)\n",
    "        self.time += self.tau\n",
    "        \n",
    "        observation = self.C.getJointState()\n",
    "        reward = self.reward_fct(C)\n",
    "        terminated = (self.time >= self.time_limit)\n",
    "        info = {\"no\": \"additional info\"}\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.time = 0.\n",
    "        self.sim.setState(X0, q0)\n",
    "        self.sim.resetSplineRef()\n",
    "\n",
    "        observation = self.C.getJointState()\n",
    "        info = {\"no\": \"additional info\"}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.C.view(False)\n",
    "\n",
    "        return observation, info\n",
    "        \n",
    "    def render(self):\n",
    "        self.C.view(False, f'RaiGym time {self.time} / {self.time_limit}')\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self.C.view_getRgb()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c78b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function\n",
    "\n",
    "def reward_function(C):\n",
    "    touch, _ = C.eval(ry.FS.negDistance, [\"stick\", \"box\"])\n",
    "    dist, _ = C.eval(ry.FS.positionDiff, [\"box\", \"target\"])\n",
    "    r = touch[0] - np.linalg.norm(dist)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58f2904-b563-440a-8be8-20226544afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = RaiGym(C, 10., reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99cf13a-44d5-4c5b-8e75-f0b5e3868f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.  0.  0.  0.  0.  0.]\n",
      "reward:  -0.5893214582361668\n",
      "reward:  -0.5747716758831056\n",
      "reward:  -0.5607135618442901\n",
      "reward:  -0.5596199361603326\n",
      "reward:  -0.5525770607798848\n",
      "reward:  -0.548713539630253\n",
      "reward:  -0.5447034660789934\n",
      "reward:  -0.5402931419992111\n",
      "reward:  -0.5354432920912546\n",
      "reward:  -0.5305754292605345\n",
      "reward:  -0.5264569238644444\n",
      "reward:  -0.5212730030586266\n",
      "reward:  -0.5164195501235808\n",
      "reward:  -0.512485752122465\n",
      "reward:  -0.5079463381830658\n",
      "reward:  -0.5038293812181991\n",
      "reward:  -0.4995472011947328\n",
      "reward:  -0.4953799975250563\n",
      "reward:  -0.4911998435791854\n",
      "reward:  -0.4868747596360802\n",
      "reward:  -0.48264148280129276\n",
      "reward:  -0.4782066427312024\n",
      "reward:  -0.47406674732598214\n",
      "reward:  -0.46974179463088245\n",
      "reward:  -0.4655479396074434\n",
      "reward:  -0.46133428881059174\n",
      "reward:  -0.456928450245985\n",
      "reward:  -0.4527467004052175\n",
      "reward:  -0.4484581731638204\n",
      "reward:  -0.44420010816304706\n",
      "reward:  -0.43987572842562545\n",
      "reward:  -0.43554870660466555\n",
      "reward:  -0.43135192519777266\n",
      "reward:  -0.42711372628805105\n",
      "reward:  -0.4227450060628551\n",
      "reward:  -0.4186111901602382\n",
      "reward:  -0.41444833365998635\n",
      "reward:  -0.41010440895994094\n",
      "reward:  -0.4059801728689506\n",
      "reward:  -0.4018458979896919\n",
      "reward:  -0.3977014417778761\n",
      "reward:  -0.3937112968328298\n",
      "reward:  -0.389678907891459\n",
      "reward:  -0.38553711760745657\n",
      "reward:  -0.3816535813467976\n",
      "reward:  -0.3777250586462601\n",
      "reward:  -0.37367441427150105\n",
      "reward:  -0.3699007023698062\n",
      "reward:  -0.3661606267785121\n",
      "reward:  -0.3623830078056325\n",
      "reward:  -0.35885206004495973\n",
      "reward:  -0.3554473311041836\n",
      "reward:  -0.3518472512616798\n",
      "reward:  -0.3486795196661083\n",
      "reward:  -0.34528863650788033\n",
      "reward:  -0.3423016736101971\n",
      "reward:  -0.3392958681579205\n",
      "reward:  -0.336466135494189\n",
      "reward:  -0.33368962338209174\n",
      "reward:  -0.33151899595951057\n",
      "reward:  -0.32932154975359823\n",
      "reward:  -0.32676526458986505\n",
      "reward:  -0.3250877976576522\n",
      "reward:  -0.3231341643109677\n",
      "reward:  -0.32149323166046034\n",
      "reward:  -0.3200432739170707\n",
      "reward:  -0.3185185955346096\n",
      "reward:  -0.3175239417824706\n",
      "reward:  -0.3164065934969522\n",
      "reward:  -0.3154834636835959\n",
      "reward:  -0.3151584579939538\n",
      "reward:  -0.3143081417018183\n",
      "reward:  -0.31430957952730415\n",
      "reward:  -0.31403429584795717\n",
      "reward:  -0.3142242502904253\n",
      "reward:  -0.3143433411977556\n",
      "reward:  -0.31446231078705167\n",
      "reward:  -0.31498131429660453\n",
      "reward:  -0.31514377173002484\n",
      "reward:  -0.3151408763871401\n",
      "reward:  -0.3155360352225836\n",
      "reward:  -0.317264215795982\n",
      "reward:  -0.32170190507757473\n",
      "reward:  -0.3258295019770395\n",
      "reward:  -0.33130496938243253\n",
      "reward:  -0.33742843086349783\n",
      "reward:  -0.34393696657201395\n",
      "reward:  -0.35081664743709695\n",
      "reward:  -0.3579543914383716\n",
      "reward:  -0.3653081186501033\n",
      "reward:  -0.37290296846578436\n",
      "reward:  -0.3805294372846611\n",
      "reward:  -0.38835335763029855\n",
      "reward:  -0.39624800987758385\n",
      "reward:  -0.4042475667942323\n",
      "reward:  -0.4123210665210753\n",
      "reward:  -0.4204645553615602\n",
      "reward:  -0.42866704288459273\n",
      "reward:  -0.4369213524266552\n",
      "reward:  -0.44522058625739563\n",
      "reward:  -0.453558688618506\n",
      "reward:  -0.4619302686392662\n",
      "reward:  -0.47033048941031325\n",
      "reward:  -0.478755128117734\n",
      "reward:  -0.48720020421721744\n",
      "reward:  -0.4956621942434132\n",
      "reward:  -0.5041378210834464\n",
      "reward:  -0.5126241711011026\n",
      "reward:  -0.5211185425810898\n",
      "reward:  -0.5296184317558587\n",
      "reward:  -0.5381215921294029\n",
      "reward:  -0.5466257579258834\n",
      "reward:  -0.5551290944132662\n",
      "reward:  -0.5636297517243379\n",
      "reward:  -0.5721259056751695\n",
      "reward:  -0.580616139841291\n",
      "reward:  -0.5890988785039327\n",
      "reward:  -0.5975727757880653\n",
      "reward:  -0.6060364189056686\n",
      "reward:  -0.6144886267115852\n",
      "reward:  -0.6229282122031484\n",
      "reward:  -0.6313541620572461\n",
      "reward:  -0.6397652471053897\n",
      "reward:  -0.6481605883409913\n",
      "reward:  -0.6565391262149594\n",
      "reward:  -0.6649000230375046\n",
      "reward:  -0.6732424132400405\n",
      "reward:  -0.6815654740003148\n",
      "reward:  -0.6898682901402147\n",
      "reward:  -0.698150103479864\n",
      "reward:  -0.7064102182096481\n",
      "reward:  -0.7146478884945711\n",
      "reward:  -0.722862308333272\n",
      "reward:  -0.7310528927002051\n",
      "reward:  -0.7392189458273771\n",
      "reward:  -0.7473597425476615\n",
      "reward:  -0.7554747463084264\n",
      "reward:  -0.7635632458790325\n",
      "reward:  -0.7716246481755572\n",
      "reward:  -0.7796583708122093\n",
      "reward:  -0.7876638357965549\n",
      "reward:  -0.7956404619031876\n",
      "reward:  -0.8035876183234612\n",
      "reward:  -0.8115047345995167\n",
      "reward:  -0.8193913071554632\n",
      "reward:  -0.8272468322771259\n",
      "reward:  -0.8350706922238619\n",
      "reward:  -0.8428623896410734\n",
      "reward:  -0.8506213755720525\n",
      "reward:  -0.8583471584322093\n",
      "reward:  -0.866039190816545\n",
      "reward:  -0.8736969908884015\n",
      "reward:  -0.8813200776848257\n",
      "reward:  -0.888907913939248\n",
      "reward:  -0.8964600240645392\n",
      "reward:  -0.9039758774938372\n",
      "reward:  -0.9114550646793509\n",
      "reward:  -0.9188971146394473\n",
      "reward:  -0.9263015062596598\n",
      "reward:  -0.9336677245691757\n",
      "reward:  -0.94099530967522\n",
      "reward:  -0.9482839161186856\n",
      "reward:  -0.9555329250254576\n",
      "reward:  -0.9627418853591375\n",
      "reward:  -0.9699105156845879\n",
      "reward:  -0.9770382058186984\n",
      "reward:  -0.9841247291990052\n",
      "reward:  -0.9911694294403686\n",
      "reward:  -0.9981718711359588\n",
      "reward:  -1.0051316216942876\n",
      "reward:  -1.0120482512365232\n",
      "reward:  -1.0189213283122793\n",
      "reward:  -1.0257505346202627\n",
      "reward:  -1.0325353425593498\n",
      "reward:  -1.0392753337200646\n",
      "reward:  -1.0459684230562476\n",
      "reward:  -1.0526148863340765\n",
      "reward:  -1.0592152862225659\n",
      "reward:  -1.065769214423645\n",
      "reward:  -1.0722762672741033\n",
      "reward:  -1.0787360417985472\n",
      "reward:  -1.0851481387358382\n",
      "reward:  -1.0915121616648324\n",
      "reward:  -1.0978278144500648\n",
      "reward:  -1.1040946090973291\n",
      "reward:  -1.1103121585643483\n",
      "reward:  -1.1164800810415068\n",
      "reward:  -1.1225979940438089\n",
      "reward:  -1.1286655203864737\n",
      "reward:  -1.1346822848352232\n",
      "reward:  -1.1406479163567114\n",
      "reward:  -1.1465620452730703\n",
      "reward:  -1.1524243581124936\n",
      "reward:  -1.158234444358972\n",
      "reward:  -1.1639919458136587\n",
      "reward:  -1.1696965056513529\n",
      "reward:  -1.1753477722939376\n",
      "reward:  -1.1809453976356656\n",
      "reward:  -1.186489035402138\n"
     ]
    }
   ],
   "source": [
    "# basic test\n",
    "\n",
    "g.reset()\n",
    "v = np.zeros(g.q0.size)\n",
    "v[0] = -1.\n",
    "print(v)\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    ret = g.step(v)\n",
    "    if ret[2]:\n",
    "        break;\n",
    "    print(\"reward: \", ret[1])\n",
    "    if not (t%10):\n",
    "        g.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee831fc-436f-461f-9644-4ad6a6516094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a stable baslines\n",
    "\n",
    "from stable_baselines3 import A2C, SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cc305e-5c9f-4798-b4e8-be5f890c54a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", g, verbose=1)\n",
    "#model = A2C(\"MlpPolicy\", g, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd665ada-a480-4e9a-a3c2-aab5dba20761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -118     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 0.706    |\n",
      "|    ent_coef        | 0.811    |\n",
      "|    ent_coef_loss   | -2.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -122     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.231    |\n",
      "|    ent_coef        | 0.638    |\n",
      "|    ent_coef_loss   | -5.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -118     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30      |\n",
      "|    critic_loss     | 4.64     |\n",
      "|    ent_coef        | 0.502    |\n",
      "|    ent_coef_loss   | -8.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -117     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.9    |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.396    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -117     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.7    |\n",
      "|    critic_loss     | 4.64     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -119     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.9    |\n",
      "|    critic_loss     | 0.763    |\n",
      "|    ent_coef        | 0.247    |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mRaiGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mControlMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvelocity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau\n\u001b[1;32m     35\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC\u001b[38;5;241m.\u001b[39mgetJointState()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a06410a-0941-48f2-851c-3d6143e3060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the policy\n",
    "\n",
    "obs, info = g.reset()\n",
    "for t in range(100):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    ret = g.step(action)\n",
    "    if not (t%10):\n",
    "        g.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fcd5153-e8a3-4612-b314-914cf19cb455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m g\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m C\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del g\n",
    "del C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c020be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
